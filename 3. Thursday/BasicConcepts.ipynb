{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Basic Probability Concepts for Random Variables \n",
    "Brief Description:  \n",
    "1) Random variables of normal distribution and Bernoulli distribution will be studied;  \n",
    "2) The probability concepts include mean, variance, $L^p$ norm ($p < \\infty$), $L^{\\infty}$ norm, covariance;  \n",
    "3) The built-in functions in numpy will be called if they exist. In addition, all the probability concepts will be defined explicitly for illustrations;  \n",
    "4) Sample statistic will be used as an estimation for the corresponding probablity concept. The accuracy will be quantified, which lies in the theory of statistical inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the packages that will be used in the file:\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Draw 50 samples from a random variable of normal distribution with mean = 0 and standard deviation = 1:\n",
    "sample_normal = np.random.normal(0,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Implement basic probability concepts for samples drawn from the standard normal distribution using numpy functions (mean and variance):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ denote a random varible. Mean of the random varible is defined as follows.  \n",
    "Definition of mean:  \n",
    "$\\mathbb{E}X := \\int_{\\Omega} X(\\omega) dP(\\omega) $, where $P$ represents the probability measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018145021797867086"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute sample mean of sample_normal using numpy:\n",
    "np.mean(sample_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 1.1.1: \n",
    "\n",
    "###### (a) Why is it reasonable to use sample mean for the estimation of the mean of the random variable? (Hint: Law of large numbers (VB, Theorem 1.3.1). More precisely, let N:= sample size, then the sample could be viewed as a realization of N independent random variables of the standard normal distribution).\n",
    "##### (b) Intuitively, the larger number of samples, the more accurate the sample mean is as an estimation for the mean of the given random variable. Can you quantify the \"accuracy\"? For example, assume that the given random variable has variance bounded by 2, how many samples are needed for the estimation error to be within 0.01 with probability 0.9? (Hint: Chebyshev's inequality / VB, Corollary 1.2.5.)\n",
    "##### (c) Is the bound provided by Chebyshev's inequality sharp? In other words, does there exist a better way to control the error so that the sample size can be smaller to obtain the estimation of comparable accuracy? (Hint: if X and Y are independent random variables of normal distribution, what is the distribution of X+Y? )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of variance:  \n",
    "Var$(X):= \\mathbb{E}(X - \\mathbb{E}X)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9603837082174097"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute sample variance of sample_normal using numpy:\n",
    "np.var(sample_normal, ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.1.2: What does \"ddof\" value represent for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Implement basic probability concepts for samples drawn from the standard normal distribution using self-defined functions (mean, variance, L^p norm, L^infinty norm, covariance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute sample mean of a sample:\n",
    "def mean(ar):\n",
    "    return np.sum(ar)/len(ar)\n",
    "\n",
    "#Compute sample variance of a sample:\n",
    "def variance(ar):\n",
    "    mean = np.sum(ar)/len(ar)\n",
    "    ar_demean = ar-mean\n",
    "    ar_demean_squared = np.square(ar_demean)\n",
    "    return np.sum(ar_demean_squared)/(len(ar)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.2.1: To compute the sample variance, why is the denominator in the last step is (the number of the sample - 1), instead of (the number of the sample)? (Hint: Bessel correction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please mimic the definitions of functions for mean and variance and define functions for $L^p$ ($p < \\infty$ ) norm and $L^\\infty$ norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of $L^p$ norm:   \n",
    "$\\|X \\|_{L^p} := (\\mathbb{E}|X|^p)^{\\frac{1}{p}}$\n",
    "\n",
    "Definition of $L^{\\infty}$ norm:  \n",
    "$\\|X \\|_{L^{\\infty}} := ess \\sup|X|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute L^p norm of a sample:\n",
    "def L_p(ar,p):\n",
    "    # Your code goes here\n",
    "\n",
    "#Compute L^infinity norm of a sample:\n",
    "def L_infinity(ar):\n",
    "    # Your code goes here\n",
    "    # Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of covariance of two random variables $X$ and $Y$:   \n",
    "$cov(X,Y):= \\mathbb{E}((X-\\mathbb{E}X)(Y-\\mathbb{E}Y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute covariance of two input samples:\n",
    "def cov(x,y):\n",
    "    if len(x) == len(y):\n",
    "        # Your code goes here\n",
    "    else:\n",
    "        print('There is an issue of missing data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.2.2: \n",
    "##### (a) Why is the denominator in the last step is (the number of the sample - 1), instead of (the number of the sample)?\n",
    "##### (b) When the issue of \"missing data\" occurs, what could be a plausible solution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please call the self-defined functions and compare the results with the ones you obtain by calling the built-in function in numpy for sample_normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the self-defined function to compute sample mean:\n",
    "#Your codes goes here\n",
    "#Use the self-defined function to compute sample variance:\n",
    "#Your codes goes here\n",
    "#Use the self-defined function to compute L^3 norm of the sample:\n",
    "#Your codes goes here\n",
    "#Use the self-defined function to compute L^infinity norm of the sample:\n",
    "#Your codes goes here\n",
    "#Use the self-defined function to compute the covariance of two samples:\n",
    "#Your codes goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw 50 samples from a random variable of Bernoulli distribution taking values 1 and -1 with probablity 1/2:\n",
    "sample_bernoulli = np.random.binomial(1,1/2,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Implement basic probability concepts for samples drawn from the Bernoulli distribution using numpy functions (mean and variance):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please call the built-in functions in numpy (for mean and variance) to compute the sample mean and sample variance for the sample_bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute sample mean of sample_bernoulli using numpy:\n",
    "# Your codes goes here\n",
    "#Compute sample variance of sample_bernoulli using numpy and self_defined function:\n",
    "# Your codes goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.1.1: \n",
    "##### (a) Can you quantify the accuracy of the sample mean as estimations for the mean of the random variable in this case? Does the same estimate in Question 1.1.1 (b) hold? Hint: One can invoke Chebyshev's inequality as in Question 1.1.1(b). \n",
    "##### (b) In the current setting, one does not know the exact distribution of the sum of the Bernoulli random variables as in Question 1.1.1. (c). However, one might recall the central limit theorem (VB, Theorem 1.3.2) and hope to approximate the distribution for the sum of the Bernoulli random variables by a normal distribution. Is this possible to render an exponential decay (of the estimation error) in the sample size? (Hint: What is the convergence rate of the central limit theorem?) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.1.1 Continued:\n",
    "##### (c) Is the bound provided by Chebyshev's inequality sharp? Is there any better estimate available in this case? (Hint: Hoeffding’s inequality / VB, Theorem 2.2.2 and Theorem 2.2.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Implement basic probability concepts for samples drawn from the standard normal distribution using self-defined functions (mean, variance, $L^p (p < \\infty)$ norm, $L^{\\infty}$ norm, covariance):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please call the self-defined functions and compare the results with the ones you obtain by calling the built-in function in numpy for sample_bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the self-defined function to compute mean of the sample:\n",
    "# Your code goes here\n",
    "#Use the self-defined function to compute variance of the sample:\n",
    "# Your code goes here\n",
    "#Use the self-defined function to compute L^4 norm of the sample:\n",
    "# Your code goes here\n",
    "#Use the self-defined function to compute L^infinity norm of the sample:\n",
    "# Your code goes here\n",
    "#Use the self-defined function to compute the covariance of two samples:\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copyright (c) 2020 TRIPODS/GradStemForAll 2020 Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
