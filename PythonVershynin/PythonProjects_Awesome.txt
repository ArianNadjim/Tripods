TRIPODS: Aug 10 through 21

Philosphy behind the program: Taking a serious book (Vershynin) and teach the mathematics behind data science. Get into a habit of coding mathematical ideas that they learn.

I'll be taking point on remediating Python knowledge. Maybe I should flesh out the elementary python folder.

Theorem 0.0.2 in Vershynin's book. Let's try it for finite sets in low dimensions, such as 1. Investigate via Python code? (Me, Ian, Arian) Think through projects that could grow out of this.

Find projects in the Chapter 0 Exercises and Theorems.

------------------------------------------

07-08-2020: Meeting with Arian and Ian

Grand Scheme for a given problem:
1) Solve with pen and paper, and get the computer to agree
2) Solve with computer what you *could* do with pen and paper that would take a while
3) Solve with neural net and computer what you could NOT do with pen and paper.

Possible Theorem 0.0.2 projects:
- Arian has python code that generates the convex hull of a randomly generated point set in two dimensions.
- Have them write code to explore the 1 dimensional problem
- Hypercube vertices in higher dimensions
- Compute the fluctuation of the error with repeated applications. Bell curve?
- relationship to Nash equilibria? application.

Possible Corollary 0.0.4 projects:
- python code to cover an interval with subintervals (opportunity for neural net optimization for point picking strategies)
- python code to cover a polygon with balls
- neural net code to distribute points on a sphere
- cover with balls that are radial basis functions instead of characteristics

Chapter 1 ideas:
- python code to simulate Theorems 1.3.1 and 1.3.2.

Chapter 2 ideas:
- visualizations for the concentration inequalities.
- guess the average degree of a graph by looking for autocorrelation in long random walks.
- random walks on a weighted graph, sampling the weights to determine some property.

Deliverables for next meeting:
Arian: writing up hypercube and polytope exercises in python
Steven: Implementing something in tensorflow and maybe the random walks?
Ian:

------------

07-12-2020: Big group meeting

Azita and Mandar joined our group:-)

second group: Xiaobo, Bai, Charlotte, Yujia, Sevak, Ustun,

Steve: flesh out the elementary python section. Ask Alex if Steve needs help.

General question (for everyone): Is Theorem 0.0.2 sharp?

General point: One big idea is to implement simplified versions of theoretical results in python. However, we need to do regular checks to make sure that these are not too simplified, so as to be disingenuous.

Student acceptance: We appear to have 10 python instructors.
15 students for TRIPODS. Shoot for 2 instructors for every 5 students, for a total of 6 instructors.
20 students for GRADSTEMFORALL. Shoot for 1 instructor for every 5 students, for a total of 4 other instructors.
python sessions will probably run between 60 and 120 minutes.

Email Alex privately if anyone (except Arian or Mandar) wants to give any particular lectures.

General meeting: Tuesday 9pm (EST)

07-14-2020: Big group meeting

ElementaryPython folder in the dropbox is just for the most basic example code and tutorials. This is not for final projects.

Scott Kirila has joined our adventure:-)

We're trying to sort out github and github desktop.

There is a python notebook with a dataframe showing which roles are assigned to whom.

Final number of participants decision will be made before we meet next week.

07-15-2020: Team GI Joe meeting (Arian, Azita, Ian, Mandar, Steve)

Arian's deliverables:
- Theorem 0.0.2 special case for hypercube
 - Maybe not very interesting to find exact solution, just linear algebra.
 - Implemented a local search algorithm, finds a local min.
 - Possibly use this to demo hillclimbing and beamsearch
 - Could we also impliment gradient descent here?
 - Ian suggested we switch to simplex, compute the answer, and compare to the random program.
 - Can we graph the cost function and show that it's convex.
 
Steve's deliverables:
- Not much. Some tensorflow tutorial code with little understanding.

Azita offered to look for some machine learning problems for students to explore.

Mandar explained a lot of stuff to us.

Ian is wondering if we could train the neural net to find good point approximations for Theorem 0.0.2.

Deliverables for next meeting:
- Arian will do the simplex approach mentioned above.
- Mandar will flesh out Arian's code into an "intro to data science algorithms"
- Steve and Ian will explore the autocorrelation in random walk on a graph, and tensorflow more.

07-21-2020: Big Meeting

starting with recaps:
GIJoe:
- basic python notebook
- approximate cartheodory python code (Arian)
- covering a polygon with random balls
- random walk on graph problems

Cobra:
- Two blackbox coinflip functions. Have students quantify which one is fair. quantify fairness. (Ustun)
- basic tools for random vectors with numpy, and chapter 3's first claim (Charlotte)
- impliment basic chapter 1 concepts, mean variance, lp norm, covariance (Yujia)

sketch: monday - first lecture (book or basic skills?), then implementation sessions - simple examples, options for followups.
as long as they're involved and as long as we have sufficient material, life will be good.

