TRIPODS: Aug 10 through 21

Philosphy behind the program: Taking a serious book (Vershynin) and teach the mathematics behind data science. Get into a habit of coding mathematical ideas that they learn.

I'll be taking point on remediating Python knowledge. Maybe I should flesh out the elementary python folder.

Theorem 0.0.2 in Vershynin's book. Let's try it for finite sets in low dimensions, such as 1. Investigate via Python code? (Me, Ian, Arian) Think through projects that could grow out of this.

Find projects in the Chapter 0 Exercises and Theorems.

------------------------------------------

07-08-2020: Meeting with Arian and Ian

Grand Scheme for a given problem:
1) Solve with pen and paper, and get the computer to agree
2) Solve with computer what you *could* do with pen and paper that would take a while
3) Solve with neural net and computer what you could NOT do with pen and paper.

Possible Theorem 0.0.2 projects:
- Arian has python code that generates the convex hull of a randomly generated point set in two dimensions.
- Have them write code to explore the 1 dimensional problem
- Hypercube vertices in higher dimensions
- Compute the fluctuation of the error with repeated applications. Bell curve?
- relationship to Nash equilibria? application.

Possible Corollary 0.0.4 projects:
- python code to cover an interval with subintervals (opportunity for neural net optimization for point picking strategies)
- python code to cover a polygon with balls
- neural net code to distribute points on a sphere
- cover with balls that are radial basis functions instead of characteristics

Chapter 1 ideas:
- python code to simulate Theorems 1.3.1 and 1.3.2.

Chapter 2 ideas:
- visualizations for the concentration inequalities.
- guess the average degree of a graph by looking for autocorrelation in long random walks.
- random walks on a weighted graph, sampling the weights to determine some property.

Deliverables for next meeting:
Arian: writing up hypercube and polytope exercises in python
Steven: Implementing something in tensorflow and maybe the random walks?
Ian:

------------

07-12-2020: Big group meeting

Azita and Mandar joined our group:-)

second group: Xiaobo, Bai, Charlotte, Yujia, Sevak, Ustun,

Steve: flesh out the elementary python section. Ask Alex if Steve needs help.

General question (for everyone): Is Theorem 0.0.2 sharp?

General point: One big idea is to implement simplified versions of theoretical results in python. However, we need to do regular checks to make sure that these are not too simplified, so as to be disingenuous.

Student acceptance: We appear to have 10 python instructors.
15 students for TRIPODS. Shoot for 2 instructors for every 5 students, for a total of 6 instructors.
20 students for GRADSTEMFORALL. Shoot for 1 instructor for every 5 students, for a total of 4 other instructors.
python sessions will probably run between 60 and 120 minutes.

Email Alex privately if anyone (except Arian or Mandar) wants to give any particular lectures.

General meeting: Tuesday 9pm (EST)

07-14-2020: Big group meeting

ElementaryPython folder in the dropbox is just for the most basic example code and tutorials. This is not for final projects.

Scott Kirila has joined our adventure:-)

We're trying to sort out github and github desktop.

There is a python notebook with a dataframe showing which roles are assigned to whom.

Final number of participants decision will be made before we meet next week.

07-15-2020: Team GI Joe meeting (Arian, Azita, Ian, Mandar, Steve)

Arian's deliverables:
- Theorem 0.0.2 special case for hypercube
 - Maybe not very interesting to find exact solution, just linear algebra.
 - Implemented a local search algorithm, finds a local min.
 - Possibly use this to demo hillclimbing and beamsearch
 - Could we also impliment gradient descent here?
 - Ian suggested we switch to simplex, compute the answer, and compare to the random program.
 - Can we graph the cost function and show that it's convex.
 
Steve's deliverables:
- Not much. Some tensorflow tutorial code with little understanding.

Azita offered to look for some machine learning problems for students to explore.

Mandar explained a lot of stuff to us.

Ian is wondering if we could train the neural net to find good point approximations for Theorem 0.0.2.

Deliverables for next meeting:
- Arian will do the simplex approach mentioned above.
- Mandar will flesh out Arian's code into an "intro to data science algorithms"
- Steve and Ian will explore the autocorrelation in random walk on a graph, and tensorflow more.

07-21-2020: Big Meeting

starting with recaps:
GIJoe:
- basic python notebook
- approximate cartheodory python code (Arian)
- covering a polygon with random balls
- random walk on graph problems

Cobra:
- Two blackbox coinflip functions. Have students quantify which one is fair. quantify fairness. visualize chebychev (Ustun)
- basic tools for random vectors with numpy, and chapter 3's first claim (Charlotte)
- impliment basic chapter 1 concepts, mean variance, lp norm, covariance (Yujia)

sketch: monday - first lecture (book or basic skills?), then implementation sessions - simple examples, options for followups.
as long as they're involved and as long as we have sufficient material, life will be good.

07-22-2020: GIJOE meeting

Arian: Implimented a simplex sampling algorithm for Thm 0.0.2. It has lots of flexibility.
Ian suggested we could have the students run this algorithm with various parameters and try to find the distribution.
Ian gave some mathematica simulations related to the random walk on the graph.
Azita said Reza recommended coding most of the tensorflow things by hand, learn more about assumptions of problems.
Azita also recommended assigning students a linear regression problem.
Steve shared his amazing ball covering code. Can students use their solutions for Thm 0.0.2 for this?

deliverables!
Mandar: Turn Arian's hypercube code into basic search algorithm tutorial.
Steve & Ian: give a well-posed problem statement for the graph walk problem.
Arian: merge approximate caratheodory and ball covering code. Look later in the book.

07-26-2020: non meeting

Arian: coding on appx cartheodory. maybe we can make an adversial point set that makes local search perform poorly?
Steve: coded some graph classes in python.
Charlotte: concentration of norm for random vectors. animation of Lp circles. kaggle datasets.

07-28-2020: big meeting

plan for first few days. Lectures will be recorded because some people will be in China.
8am-9am - first lecture
10am-11am - second lecture
2pm-3:30pm - python (We'll have code for them to play with. They should generate projects.)
7:30pm-9pm - recitation

Monday:
- 8am-9am: appetizer lecture (Alex)
- 10am-11am: Basic probability skills (Sevak)

Tuesday:
- 8am-9am: Vershynin (Alex)
- 10am-11am: More basic probability (Sevak)

Steve might want to ask Ethan Lynch to be a python TA.
Mandar might be setting up a discord server for people to work together.
Perhaps some sort of social events on the weekends.

07-29-2020: GIJOE

Steve: random walk on graph might have tricky "probability space." Maybe we start with a cycle, then add some other edges.
Ian: Watts-Strogatz model should help. Barabasi Albert model?
Graph problems: Better first question: What's the probability we hit each vertex in a random walk on a compleete graph? cycle?
Then we ask the kids to come up with a "random graph" question. They might ask us to be more precise about "random graph," and THEN
we tell them about the various known models. Maybe they could model disease spreading?

Mandar made a beautiful jupyter notebook guiding students through Arian's search code.

Arian coded up a visualization for Corollary 0.0.4. Maybe we could grab some small subset of the N^k points (ball centers) at random, and ask how efficient that covering is? His code currently computes all possible points iteratively, but maybe we could compute fewer, and perhaps have some criteria for picking "good" point sets?

Deliverables:

Mandar might code up a linear regression python demo. Also maybe the house price dataset thing.
Arian might implement something from Chapter 10, and might work on his ball covering code some more.
Steve and Ian will work on the random walk problem.
All of us might make more lab things like Mandar made.